{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ST-RESNET.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"GQnx8ZhC4zBN","executionInfo":{"status":"ok","timestamp":1602341444074,"user_tz":-120,"elapsed":46901,"user":{"displayName":"Roberto Tagliabue","photoUrl":"","userId":"09447467556359440393"}},"outputId":"874453ee-ec4c-4076-9ae8-9c6f930bdd41","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip install q keras==1.2.2\n","!pip install tensorflow==1.4"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting q\n","  Downloading https://files.pythonhosted.org/packages/53/bc/51619d89e0bd855567e7652fa16d06f1ed36a85f108a7fe71f6629bf719d/q-2.6-py2.py3-none-any.whl\n","Collecting keras==1.2.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/3e/9926ce5c678b7a7978724a2ecf24857d89a415d152b8d3443e6d45c228b2/Keras-1.2.2.tar.gz (175kB)\n","\r\u001b[K     |█▉                              | 10kB 15.6MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20kB 15.3MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 112kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 122kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 163kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 174kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 6.9MB/s \n","\u001b[?25hRequirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.0.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (3.13)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.15.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from theano->keras==1.2.2) (1.18.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from theano->keras==1.2.2) (1.4.1)\n","Building wheels for collected packages: keras\n","  Building wheel for keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras: filename=Keras-1.2.2-cp36-none-any.whl size=209603 sha256=faa243cdc9c3d8471911f439dbfd345cc403f92f599ea631b0234c7fca3058c4\n","  Stored in directory: /root/.cache/pip/wheels/55/07/cf/b32db0a8d243b2fd6759d5d7cb650aa20670b2b740209cbf7e\n","Successfully built keras\n","\u001b[31mERROR: textgenrnn 1.4.1 has requirement keras>=2.1.5, but you'll have keras 1.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.1.3.1 has requirement keras>=2.0.0, but you'll have keras 1.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fancyimpute 0.4.3 has requirement keras>=2.0.0, but you'll have keras 1.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: q, keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed keras-1.2.2 q-2.6\n","Collecting tensorflow==1.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9f/be0165c6eefd841e6928e54d3d083fa174f92d640fdc52f73a33dc9c54d1/tensorflow-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (41.2MB)\n","\u001b[K     |████████████████████████████████| 41.2MB 73kB/s \n","\u001b[?25hCollecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/9f/5845c18f9df5e7ea638ecf3a272238f0e7671e454faa396b5188c6e6fc0a/tensorflow_tensorboard-0.4.0-py3-none-any.whl (1.7MB)\n","\u001b[K     |████████████████████████████████| 1.7MB 39.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4) (1.18.5)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4) (1.15.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4) (0.35.1)\n","Requirement already satisfied: protobuf>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4) (3.12.4)\n","Collecting enum34>=1.1.6\n","  Downloading https://files.pythonhosted.org/packages/63/f6/ccb1c83687756aeabbf3ca0f213508fcfb03883ff200d201b3a4c60cedcc/enum34-1.1.10-py3-none-any.whl\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) (3.2.2)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) (1.0.1)\n","Collecting html5lib==0.9999999\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n","\u001b[K     |████████████████████████████████| 890kB 26.5MB/s \n","\u001b[?25hCollecting bleach==1.5.0\n","  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.3.0->tensorflow==1.4) (50.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) (2.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) (3.2.0)\n","Building wheels for collected packages: html5lib\n","  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=132048a6c7167935d73acab8ccdf15dda7033a168a4b1bbbbee9276f6f9182c5\n","  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n","Successfully built html5lib\n","\u001b[31mERROR: fancyimpute 0.4.3 has requirement keras>=2.0.0, but you'll have keras 1.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: html5lib, bleach, tensorflow-tensorboard, enum34, tensorflow\n","  Found existing installation: html5lib 1.0.1\n","    Uninstalling html5lib-1.0.1:\n","      Successfully uninstalled html5lib-1.0.1\n","  Found existing installation: bleach 3.2.1\n","    Uninstalling bleach-3.2.1:\n","      Successfully uninstalled bleach-3.2.1\n","  Found existing installation: tensorflow 2.3.0\n","    Uninstalling tensorflow-2.3.0:\n","      Successfully uninstalled tensorflow-2.3.0\n","Successfully installed bleach-1.5.0 enum34-1.1.10 html5lib-0.9999999 tensorflow-1.4.0 tensorflow-tensorboard-0.4.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["enum"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"lMU5_4P773JY","executionInfo":{"status":"ok","timestamp":1602341470559,"user_tz":-120,"elapsed":24588,"user":{"displayName":"Roberto Tagliabue","photoUrl":"","userId":"09447467556359440393"}},"outputId":"c407f79f-86fa-46c9-e466-abfd3469648f","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\", force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e71oaU_u7FLn","executionInfo":{"status":"ok","timestamp":1602341474144,"user_tz":-120,"elapsed":1020,"user":{"displayName":"Roberto Tagliabue","photoUrl":"","userId":"09447467556359440393"}}},"source":["import os\n","os.chdir('./drive/My Drive/TESI/ST-ResNet')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLvEqkOH9ENh","executionInfo":{"status":"ok","timestamp":1602341486538,"user_tz":-120,"elapsed":10010,"user":{"displayName":"Roberto Tagliabue","photoUrl":"","userId":"09447467556359440393"}},"outputId":"bedab0b2-141a-43f7-ea79-eb3b2475164a","colab":{"base_uri":"https://localhost:8080/","height":292}},"source":["from __future__ import print_function\n","import os\n","import sys\n","import pickle\n","import time\n","import numpy as np\n","import h5py\n","\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","from deepst.models.STResNet import stresnet\n","from deepst.config import Config\n","import deepst.metrics as metrics\n","from deepst.datasets import TaxiBJ"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n","  return f(*args, **kwds)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"4j6PUu-u7gYz","executionInfo":{"status":"ok","timestamp":1602248885646,"user_tz":-120,"elapsed":778,"user":{"displayName":"Roberto Tagliabue","photoUrl":"","userId":"09447467556359440393"}},"outputId":"e76f3cd3-2d7b-4f6c-bc52-a26d8685e55f","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import keras\n","import tensorflow as tf\n","print(keras.__version__)\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.2.2\n","1.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wM2H3NeJ75X5","executionInfo":{"status":"ok","timestamp":1602341486544,"user_tz":-120,"elapsed":4451,"user":{"displayName":"Roberto Tagliabue","photoUrl":"","userId":"09447467556359440393"}}},"source":["np.random.seed(1337)  # for reproducibility\n","\n","# parameters\n","DATAPATH = '../data'  # data path, you may set your own data path with the global envirmental variable DATAPATH\n","CACHEDATA = True  # cache data or NOT\n","path_cache = os.path.join(DATAPATH, 'CACHE', 'ST-ResNet')  # cache path\n","nb_epoch = 500  # number of epoch at training stage\n","nb_epoch_cont = 100  # number of epoch at training (cont) stage\n","batch_size = 32  # batch size\n","T = 48  # number of time intervals in one day\n","lr = 0.0002  # learning rate\n","len_closeness = 3  # length of closeness dependent sequence\n","len_period = 1  # length of peroid dependent sequence\n","len_trend = 1  # length of trend dependent sequence\n","nb_residual_unit = 12 # paper says 12 for taxiBJ\n","\n","nb_flow = 2  # there are two types of flows: inflow and outflow\n","# divide data into two subsets: Train & Test, of which the test set is the\n","# last 4 weeks\n","days_test = 7 * 4\n","len_test = T * days_test\n","map_height, map_width = 32, 32  # grid size\n","path_result = 'RET'\n","path_model = 'MODEL'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"UpNVjgT875ai","executionInfo":{"status":"ok","timestamp":1602341489085,"user_tz":-120,"elapsed":1528,"user":{"displayName":"Roberto Tagliabue","photoUrl":"","userId":"09447467556359440393"}}},"source":["if os.path.isdir(path_result) is False:\n","    os.mkdir(path_result)\n","if os.path.isdir(path_model) is False:\n","    os.mkdir(path_model)\n","if CACHEDATA and os.path.isdir(path_cache) is False:\n","    os.mkdir(path_cache)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"3XjJiPHP75dY"},"source":["def build_model(external_dim):\n","    c_conf = (len_closeness, nb_flow, map_height,\n","              map_width) if len_closeness > 0 else None\n","    p_conf = (len_period, nb_flow, map_height,\n","              map_width) if len_period > 0 else None\n","    t_conf = (len_trend, nb_flow, map_height,\n","              map_width) if len_trend > 0 else None\n","    model = stresnet(c_conf=c_conf, p_conf=p_conf, t_conf=t_conf,\n","                     external_dim=external_dim, nb_residual_unit=nb_residual_unit)\n","    adam = Adam(lr=lr)\n","    model.compile(loss='mse', optimizer=adam, metrics=[metrics.rmse])\n","    model.summary()\n","    # from keras.utils.visualize_util import plot\n","    # plot(model, to_file='model.png', show_shapes=True)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u80V_X5W75gi","executionInfo":{"status":"ok","timestamp":1602341495639,"user_tz":-120,"elapsed":1254,"user":{"displayName":"Roberto Tagliabue","photoUrl":"","userId":"09447467556359440393"}}},"source":["def read_cache(fname):\n","    mmn = pickle.load(open('preprocessing.pkl', 'rb'))\n","\n","    f = h5py.File(fname, 'r')\n","    num = int(f['num'].value)\n","    X_train, Y_train, X_test, Y_test = [], [], [], []\n","    for i in range(num):\n","        X_train.append(f['X_train_%i' % i].value)\n","        X_test.append(f['X_test_%i' % i].value)\n","    Y_train = f['Y_train'].value\n","    Y_test = f['Y_test'].value\n","    external_dim = f['external_dim'].value\n","    timestamp_train = f['T_train'].value\n","    timestamp_test = f['T_test'].value\n","    f.close()\n","\n","    return X_train, Y_train, X_test, Y_test, mmn, external_dim, timestamp_train, timestamp_test"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"J7cDAfSW75jY","executionInfo":{"status":"ok","timestamp":1602341497814,"user_tz":-120,"elapsed":1387,"user":{"displayName":"Roberto Tagliabue","photoUrl":"","userId":"09447467556359440393"}}},"source":["def cache(fname, X_train, Y_train, X_test, Y_test, external_dim, timestamp_train, timestamp_test):\n","    h5 = h5py.File(fname, 'w')\n","    h5.create_dataset('num', data=len(X_train))\n","\n","    for i, data in enumerate(X_train):\n","        h5.create_dataset('X_train_%i' % i, data=data)\n","    # for i, data in enumerate(Y_train):\n","    for i, data in enumerate(X_test):\n","        h5.create_dataset('X_test_%i' % i, data=data)\n","    h5.create_dataset('Y_train', data=Y_train)\n","    h5.create_dataset('Y_test', data=Y_test)\n","    external_dim = -1 if external_dim is None else int(external_dim)\n","    h5.create_dataset('external_dim', data=external_dim)\n","    h5.create_dataset('T_train', data=timestamp_train)\n","    h5.create_dataset('T_test', data=timestamp_test)\n","    h5.close()"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"CvplIrhn75os","executionInfo":{"status":"ok","timestamp":1602341513657,"user_tz":-120,"elapsed":12993,"user":{"displayName":"Roberto Tagliabue","photoUrl":"","userId":"09447467556359440393"}},"outputId":"afea98a1-7a47-4d28-c5db-9776dd10bab7","colab":{"base_uri":"https://localhost:8080/","height":445}},"source":["print(\"loading data...\")\n","ts = time.time()\n","fname = os.path.join(path_cache, 'TaxiBJ_C{}_P{}_T{}.h5'.format(\n","    len_closeness, len_period, len_trend))\n","if os.path.exists(fname) and CACHEDATA:\n","    X_train, Y_train, X_test, Y_test, mmn, external_dim, timestamp_train, timestamp_test = read_cache(\n","        fname)\n","    print(\"load %s successfully\" % fname)\n","else:\n","    X_train, Y_train, X_test, Y_test, mmn, external_dim, timestamp_train, timestamp_test = TaxiBJ.load_data(\n","        T=T, nb_flow=nb_flow, len_closeness=len_closeness, len_period=len_period, len_trend=len_trend, len_test=len_test,\n","        preprocess_name='preprocessing.pkl', meta_data=True, meteorol_data=False, holiday_data=False, datapath=DATAPATH)\n","    if CACHEDATA:\n","        cache(fname, X_train, Y_train, X_test, Y_test,\n","              external_dim, timestamp_train, timestamp_test)\n","\n","print(\"\\n days (test): \", [v[:8] for v in timestamp_test[0::T]])\n","print(\"\\nelapsed time (loading data): %.3f seconds\\n\" % (time.time() - ts))\n","\n","print('=' * 10)\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["loading data...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  if __name__ == '__main__':\n"],"name":"stderr"},{"output_type":"stream","text":["load ../data/CACHE/ST-ResNet/TaxiBJ_C3_P1_T1.h5 successfully\n","\n"," days (test):  [b'20160310', b'20160311', b'20160312', b'20160313', b'20160314', b'20160315', b'20160316', b'20160317', b'20160318', b'20160319', b'20160320', b'20160321', b'20160322', b'20160325', b'20160326', b'20160327', b'20160328', b'20160329', b'20160331', b'20160401', b'20160402', b'20160403', b'20160404', b'20160405', b'20160406', b'20160407', b'20160408', b'20160409']\n","\n","elapsed time (loading data): 11.914 seconds\n","\n","==========\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  # Remove the CWD from sys.path while we load stuff.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  if sys.path[0] == '':\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  del sys.path[0]\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"rr0-8Bod6vK9","executionInfo":{"status":"ok","timestamp":1602341519395,"user_tz":-120,"elapsed":2182,"user":{"displayName":"Roberto Tagliabue","photoUrl":"","userId":"09447467556359440393"}},"outputId":"c3c4c938-a068-495f-a0d2-e7f59bc994a2","colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["#from deepst.datasets import stat; stat('BJ16_M32x32_T30_InOut.h5')\n","f = h5py.File('../data/TaxiBJ/BJ15_M32x32_T30_InOut.h5', 'r')\n","print(len(f['date']))\n","# print(f['data'][1][0][0])\n","\n","print(len(X_train))\n","print(len(X_train[0]))\n","print(len(X_train[0][0]))\n","xtrain_np = np.array(X_train[0], dtype=np.float32)\n","print(xtrain_np.shape)\n","print(len(Y_train))\n","ytrain_np = np.array(Y_train, dtype=np.float32)\n","print(ytrain_np.shape)\n","print(len(timestamp_train))\n","print(len(timestamp_test))\n","print(len(X_test))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["5596\n","4\n","13728\n","6\n","(13728, 6, 32, 32)\n","13728\n","(13728, 2, 32, 32)\n","13728\n","1344\n","4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uNo0JYIGFPME","executionInfo":{"status":"ok","timestamp":1602249548673,"user_tz":-120,"elapsed":3065,"user":{"displayName":"Roberto Tagliabue","photoUrl":"","userId":"09447467556359440393"}},"outputId":"a2a6f5f4-42d4-45bc-f81e-df1337aaa77d","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["print(\"compiling model...\")\n","print(\n","    \"**at the first time, it takes a few minites to compile if you use [Theano] as the backend**\")\n","ts = time.time()\n","model = build_model(external_dim)\n","hyperparams_name = 'TaxiBJ.c{}.p{}.t{}.resunit{}.lr{}'.format(\n","    len_closeness, len_period, len_trend, nb_residual_unit, lr)\n","fname_param = os.path.join('MODEL', '{}.best.h5'.format(hyperparams_name))\n","\n","early_stopping = EarlyStopping(monitor='val_rmse', patience=2, mode='min')\n","model_checkpoint = ModelCheckpoint(\n","    fname_param, monitor='val_rmse', verbose=0, save_best_only=True, mode='min')\n","\n","print(\"\\nelapsed time (compiling model): %.3f seconds\\n\" %\n","      (time.time() - ts))\n","\n","print('=' * 10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["compiling model...\n","**at the first time, it takes a few minites to compile if you use [Theano] as the backend**\n","(?, 2, 32, 32)\n","(?, 2, 32, 32)\n","(?, 2, 32, 32)\n","____________________________________________________________________________________________________\n","Layer (type)                     Output Shape          Param #     Connected to                     \n","====================================================================================================\n","input_1 (InputLayer)             (None, 6, 32, 32)     0                                            \n","____________________________________________________________________________________________________\n","input_2 (InputLayer)             (None, 2, 32, 32)     0                                            \n","____________________________________________________________________________________________________\n","input_3 (InputLayer)             (None, 2, 32, 32)     0                                            \n","____________________________________________________________________________________________________\n","convolution2d_1 (Convolution2D)  (None, 64, 32, 32)    3520        input_1[0][0]                    \n","____________________________________________________________________________________________________\n","convolution2d_7 (Convolution2D)  (None, 64, 32, 32)    1216        input_2[0][0]                    \n","____________________________________________________________________________________________________\n","convolution2d_13 (Convolution2D) (None, 64, 32, 32)    1216        input_3[0][0]                    \n","____________________________________________________________________________________________________\n","activation_1 (Activation)        (None, 64, 32, 32)    0           convolution2d_1[0][0]            \n","____________________________________________________________________________________________________\n","activation_6 (Activation)        (None, 64, 32, 32)    0           convolution2d_7[0][0]            \n","____________________________________________________________________________________________________\n","activation_11 (Activation)       (None, 64, 32, 32)    0           convolution2d_13[0][0]           \n","____________________________________________________________________________________________________\n","convolution2d_2 (Convolution2D)  (None, 64, 32, 32)    36928       activation_1[0][0]               \n","____________________________________________________________________________________________________\n","convolution2d_8 (Convolution2D)  (None, 64, 32, 32)    36928       activation_6[0][0]               \n","____________________________________________________________________________________________________\n","convolution2d_14 (Convolution2D) (None, 64, 32, 32)    36928       activation_11[0][0]              \n","____________________________________________________________________________________________________\n","activation_2 (Activation)        (None, 64, 32, 32)    0           convolution2d_2[0][0]            \n","____________________________________________________________________________________________________\n","activation_7 (Activation)        (None, 64, 32, 32)    0           convolution2d_8[0][0]            \n","____________________________________________________________________________________________________\n","activation_12 (Activation)       (None, 64, 32, 32)    0           convolution2d_14[0][0]           \n","____________________________________________________________________________________________________\n","convolution2d_3 (Convolution2D)  (None, 64, 32, 32)    36928       activation_2[0][0]               \n","____________________________________________________________________________________________________\n","convolution2d_9 (Convolution2D)  (None, 64, 32, 32)    36928       activation_7[0][0]               \n","____________________________________________________________________________________________________\n","convolution2d_15 (Convolution2D) (None, 64, 32, 32)    36928       activation_12[0][0]              \n","____________________________________________________________________________________________________\n","merge_1 (Merge)                  (None, 64, 32, 32)    0           convolution2d_1[0][0]            \n","                                                                   convolution2d_3[0][0]            \n","____________________________________________________________________________________________________\n","merge_3 (Merge)                  (None, 64, 32, 32)    0           convolution2d_7[0][0]            \n","                                                                   convolution2d_9[0][0]            \n","____________________________________________________________________________________________________\n","merge_5 (Merge)                  (None, 64, 32, 32)    0           convolution2d_13[0][0]           \n","                                                                   convolution2d_15[0][0]           \n","____________________________________________________________________________________________________\n","activation_3 (Activation)        (None, 64, 32, 32)    0           merge_1[0][0]                    \n","____________________________________________________________________________________________________\n","activation_8 (Activation)        (None, 64, 32, 32)    0           merge_3[0][0]                    \n","____________________________________________________________________________________________________\n","activation_13 (Activation)       (None, 64, 32, 32)    0           merge_5[0][0]                    \n","____________________________________________________________________________________________________\n","convolution2d_4 (Convolution2D)  (None, 64, 32, 32)    36928       activation_3[0][0]               \n","____________________________________________________________________________________________________\n","convolution2d_10 (Convolution2D) (None, 64, 32, 32)    36928       activation_8[0][0]               \n","____________________________________________________________________________________________________\n","convolution2d_16 (Convolution2D) (None, 64, 32, 32)    36928       activation_13[0][0]              \n","____________________________________________________________________________________________________\n","activation_4 (Activation)        (None, 64, 32, 32)    0           convolution2d_4[0][0]            \n","____________________________________________________________________________________________________\n","activation_9 (Activation)        (None, 64, 32, 32)    0           convolution2d_10[0][0]           \n","____________________________________________________________________________________________________\n","activation_14 (Activation)       (None, 64, 32, 32)    0           convolution2d_16[0][0]           \n","____________________________________________________________________________________________________\n","convolution2d_5 (Convolution2D)  (None, 64, 32, 32)    36928       activation_4[0][0]               \n","____________________________________________________________________________________________________\n","convolution2d_11 (Convolution2D) (None, 64, 32, 32)    36928       activation_9[0][0]               \n","____________________________________________________________________________________________________\n","convolution2d_17 (Convolution2D) (None, 64, 32, 32)    36928       activation_14[0][0]              \n","____________________________________________________________________________________________________\n","input_4 (InputLayer)             (None, 8)             0                                            \n","____________________________________________________________________________________________________\n","merge_2 (Merge)                  (None, 64, 32, 32)    0           merge_1[0][0]                    \n","                                                                   convolution2d_5[0][0]            \n","____________________________________________________________________________________________________\n","merge_4 (Merge)                  (None, 64, 32, 32)    0           merge_3[0][0]                    \n","                                                                   convolution2d_11[0][0]           \n","____________________________________________________________________________________________________\n","merge_6 (Merge)                  (None, 64, 32, 32)    0           merge_5[0][0]                    \n","                                                                   convolution2d_17[0][0]           \n","____________________________________________________________________________________________________\n","dense_1 (Dense)                  (None, 10)            90          input_4[0][0]                    \n","____________________________________________________________________________________________________\n","activation_5 (Activation)        (None, 64, 32, 32)    0           merge_2[0][0]                    \n","____________________________________________________________________________________________________\n","activation_10 (Activation)       (None, 64, 32, 32)    0           merge_4[0][0]                    \n","____________________________________________________________________________________________________\n","activation_15 (Activation)       (None, 64, 32, 32)    0           merge_6[0][0]                    \n","____________________________________________________________________________________________________\n","activation_16 (Activation)       (None, 10)            0           dense_1[0][0]                    \n","____________________________________________________________________________________________________\n","convolution2d_6 (Convolution2D)  (None, 2, 32, 32)     1154        activation_5[0][0]               \n","____________________________________________________________________________________________________\n","convolution2d_12 (Convolution2D) (None, 2, 32, 32)     1154        activation_10[0][0]              \n","____________________________________________________________________________________________________\n","convolution2d_18 (Convolution2D) (None, 2, 32, 32)     1154        activation_15[0][0]              \n","____________________________________________________________________________________________________\n","dense_2 (Dense)                  (None, 2048)          22528       activation_16[0][0]              \n","____________________________________________________________________________________________________\n","ilayer_1 (iLayer)                (None, 2, 32, 32)     2048        convolution2d_6[0][0]            \n","____________________________________________________________________________________________________\n","ilayer_2 (iLayer)                (None, 2, 32, 32)     2048        convolution2d_12[0][0]           \n","____________________________________________________________________________________________________\n","ilayer_3 (iLayer)                (None, 2, 32, 32)     2048        convolution2d_18[0][0]           \n","____________________________________________________________________________________________________\n","activation_17 (Activation)       (None, 2048)          0           dense_2[0][0]                    \n","____________________________________________________________________________________________________\n","merge_7 (Merge)                  (None, 2, 32, 32)     0           ilayer_1[0][0]                   \n","                                                                   ilayer_2[0][0]                   \n","                                                                   ilayer_3[0][0]                   \n","____________________________________________________________________________________________________\n","reshape_1 (Reshape)              (None, 2, 32, 32)     0           activation_17[0][0]              \n","____________________________________________________________________________________________________\n","merge_8 (Merge)                  (None, 2, 32, 32)     0           merge_7[0][0]                    \n","                                                                   reshape_1[0][0]                  \n","____________________________________________________________________________________________________\n","activation_18 (Activation)       (None, 2, 32, 32)     0           merge_8[0][0]                    \n","====================================================================================================\n","Total params: 481,312\n","Trainable params: 481,312\n","Non-trainable params: 0\n","____________________________________________________________________________________________________\n","\n","elapsed time (compiling model): 2.214 seconds\n","\n","==========\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JM9_IuaJ75ry","executionInfo":{"status":"error","timestamp":1602249869902,"user_tz":-120,"elapsed":71285,"user":{"displayName":"Roberto Tagliabue","photoUrl":"","userId":"09447467556359440393"}},"outputId":"279d7427-770e-42e9-8c6f-3463399e98cd","colab":{"base_uri":"https://localhost:8080/","height":459}},"source":["print(\"training model...\")\n","ts = time.time()\n","history = model.fit(X_train, Y_train,\n","                    nb_epoch=nb_epoch,\n","                    batch_size=batch_size,\n","                    validation_split=0.1,\n","                    callbacks=[early_stopping, model_checkpoint],\n","                    verbose=1)\n","model.save_weights(os.path.join(\n","    'MODEL', '{}.h5'.format(hyperparams_name)), overwrite=True)\n","pickle.dump((history.history), open(os.path.join(\n","    path_result, '{}.history.pkl'.format(hyperparams_name)), 'wb'))\n","print(\"\\nelapsed time (training): %.3f seconds\\n\" % (time.time() - ts))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["training model...\n","Train on 12355 samples, validate on 1373 samples\n","Epoch 1/500\n","  192/12355 [..............................] - ETA: 3797s - loss: 0.3825 - rmse: 0.5791"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-d64eaed38b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     verbose=1)\n\u001b[0m\u001b[1;32m      9\u001b[0m model.save_weights(os.path.join(\n\u001b[1;32m     10\u001b[0m     'MODEL', '{}.h5'.format(hyperparams_name)), overwrite=True)\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1194\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1941\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1943\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1944\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"aN6sU4hk75ux","executionInfo":{"status":"ok","timestamp":1602261731962,"user_tz":-120,"elapsed":147561,"user":{"displayName":"Roberto Tagliabue","photoUrl":"","userId":"09447467556359440393"}},"outputId":"d9e6196f-cbd5-41f4-95c3-9e6c5fb669a5","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python main_taxiBJ.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n","  return f(*args, **kwds)\n","loading data...\n","main_taxiBJ.py:64: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  num = int(f['num'].value)\n","main_taxiBJ.py:67: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  X_train.append(f['X_train_%i' % i].value)\n","main_taxiBJ.py:68: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  X_test.append(f['X_test_%i' % i].value)\n","main_taxiBJ.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  Y_train = f['Y_train'].value\n","main_taxiBJ.py:70: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  Y_test = f['Y_test'].value\n","main_taxiBJ.py:71: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  external_dim = f['external_dim'].value\n","main_taxiBJ.py:72: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  timestamp_train = f['T_train'].value\n","main_taxiBJ.py:73: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  timestamp_test = f['T_test'].value\n","load ../data/CACHE/TaxiBJ_C3_P1_T1.h5 successfully\n","\n"," days (test):  [b'20160310', b'20160311', b'20160312', b'20160313', b'20160314', b'20160315', b'20160316', b'20160317', b'20160318', b'20160319', b'20160320', b'20160321', b'20160322', b'20160325', b'20160326', b'20160327', b'20160328', b'20160329', b'20160331', b'20160401', b'20160402', b'20160403', b'20160404', b'20160405', b'20160406', b'20160407', b'20160408', b'20160409']\n","\n","elapsed time (loading data): 20.447 seconds\n","\n","==========\n","compiling model...\n","**at the first time, it takes a few minites to compile if you use [Theano] as the backend**\n","(?, 2, 32, 32)\n","(?, 2, 32, 32)\n","(?, 2, 32, 32)\n","\n","elapsed time (compiling model): 1.342 seconds\n","\n","==========\n","training model...\n","Train on 12355 samples, validate on 1373 samples\n","Epoch 1/500\n","2020-10-09 16:40:22.106718: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n","   32/12355 [..............................] - ETA: 21587s - loss: 0.8935 - rmse: 0.9452Traceback (most recent call last):\n","  File \"main_taxiBJ.py\", line 142, in <module>\n","    verbose=1)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1196, in fit\n","    initial_epoch=initial_epoch)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 891, in _fit_loop\n","    outs = f(ins_batch)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 1943, in __call__\n","    feed_dict=feed_dict)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 889, in run\n","    run_metadata_ptr)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run\n","    feed_dict_tensor, options, run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n","    options, run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n","    return fn(*args)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n","    status, run_metadata)\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5dNwrgQV75xu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0SWsU0nu750o"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jGYtOOI-753h"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KN-yW96o756m"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LXPKPInc759o"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FWFJTSR476Ax"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LB9J6ihx76Dw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M7urQno976Gx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jbDDUZGB76Kw"},"source":[""],"execution_count":null,"outputs":[]}]}